{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsdKToFor69U"
      },
      "source": [
        "# Homework 3, exercise 2 - Residual Neural Network on CIFAR10\n",
        "\n",
        "In this exercise we implement a (slightly modified) ResNet as introduced in [this paper](https://arxiv.org/pdf/1512.03385.pdf).\n",
        "\n",
        "We will use the CIFAR-10 dataset and we will implement and train a ResNet to properly classifying the input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VdY58D3KMZO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRuR6CcbsW8_"
      },
      "source": [
        "For this exercise it is recommended to use the GPU! The ResNet uses conv2d layers and skip connections, making the training very slow on CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZQhrlxKSTK",
        "outputId": "234e4ead-df08-4059-90c6-ecef88a5dbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwJz3i37UXsZ"
      },
      "source": [
        "### Load the CIFAR10 dataset\n",
        "\n",
        "The CIFAR10 dataset is composed of 32x32x3 (height x width x channel) labeled images belonging to 10 different classes ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1WVamZiKSXR",
        "outputId": "e2a0d571-42fa-49a9-eb64-ff86bdd9ddce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data_cifar', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data_cifar', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "c, w, h = 3, 32, 32\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 60408394.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data_cifar/cifar-10-python.tar.gz to ./data_cifar\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfpdVQRbUg5p"
      },
      "source": [
        "## Exercise 1 - Implement a Residual Block\n",
        "\n",
        "Residual neural networks mainly consist of components called Residual Blocks. One residual block can be expressed as **y** = *f*(**x**) + **x** (see Equation (11.5)), where **x** and **y** are the input and output of the block, respectively. So the input **x** is added to the result of *f*(**x**) using a *skip connection*.\n",
        "\n",
        "In this exercise, *f* consists of:\n",
        "1. a 2d convolutional layer with input channels=`in_channels`, output channels=`hidden_channels`, a kernel size of (3, 3), a stride of 1, padding of 1 and no bias parameter.\n",
        "2. a batch normalisation layer\n",
        "3. ReLU activation\n",
        "4. a 2d convolutional layer with input channels=`hidden_channels`, output channels=`out_channels`, a kernel size of (3, 3), a stride of 1, padding of 1 and no bias parameter.\n",
        "5. a batch normalisation layer\n",
        "\n",
        "After this the `skip_connection` is applied. If the dimensions of *f*(**x**) and **x** do not match, an extra linear projection is applied to **x** so the dimensions match. This has already been implemented for you. You only need to call it at the right place.\n",
        "Finally, a ReLU activation is applied on the output **y**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK1qpjYwUFqh"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    # TODO: Define the layers\n",
        "\n",
        "    self.conv1 = # TODO\n",
        "    self.batch1 = # TODO\n",
        "    self.relu = # TODO\n",
        "    self.conv2 = # TODO\n",
        "    self.batch2 = # TODO\n",
        "    ###############################################################\n",
        "\n",
        "        if in_channels != out_channels:  # f(x) and x dimensions do not match! Define a projection for input x\n",
        "      self.skip_connection = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    else:\n",
        "      self.skip_connection = lambda x: x  # The dimensions already match! No need to do a projection on x\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: Implement the forward pass\n",
        "    skip = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    return x\n",
        "    ###############################################################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Y87D77cYX8"
      },
      "source": [
        "## Exercise 2 - Implement a Residual Neural Network\n",
        "Now you can use the previously defined Residual Block to create your ResNet.\n",
        "\n",
        "The network consists of:\n",
        "1. a convolutional layer with input channels=`in_channels`, output channels=64, a stride of 1, padding of 1 and no bias parameter,\n",
        "2. a batch normalisation layer\n",
        "3. ReLU activation\n",
        "4. a max pooling layer with kernel size (3, 3), a stride of 2 and padding of 1,\n",
        "5. eight residual blocks, with (64, 64, 128, 128, 256, 256, 512, 512) channels, respectively (see code below)\n",
        "6. an average pooling layer over all feature maps (already present)\n",
        "7. a dense layer to form the output distribution (already present)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qVgN9lPKSeC"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # TODO: Define the layers\n",
        "\n",
        "    self.conv1 = # TODO\n",
        "    self.batch1 = # TODO\n",
        "    self.relu = # TODO\n",
        "    self.pool1 = #TODO\n",
        "\n",
        "    ###############################################################\n",
        "\n",
        "    self.res_blocks = nn.ModuleList(\n",
        "        [\n",
        "         ResidualBlock(64, 64, 64),\n",
        "         ResidualBlock(64, 64, 64),\n",
        "\n",
        "         ResidualBlock(64, 128, 128),\n",
        "         ResidualBlock(128, 128, 128),\n",
        "\n",
        "         ResidualBlock(128, 256, 256),\n",
        "         ResidualBlock(256, 256, 256),\n",
        "\n",
        "         ResidualBlock(256, 512, 512),\n",
        "         ResidualBlock(512, 512, 512),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.dense_layer = nn.Linear(512, out_size)\n",
        "\n",
        "    for module in self.modules():\n",
        "      if isinstance(module, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # TODO: Implement the forward pass (add everything that needs to be done before the average pooling)\n",
        "\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    x = # TODO\n",
        "    for block in self.res_blocks:\n",
        "      x = # TODO\n",
        "\n",
        "    #################################################################\n",
        "\n",
        "    x = F.avg_pool2d(x, x.shape[2:])\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.dense_layer(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9ny4USgNAu"
      },
      "source": [
        "### Initialize the network, Loss function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIofWmkrT6Oh"
      },
      "source": [
        "net = ResNet(c, len(classes)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojw0pS0dgZHX"
      },
      "source": [
        "## Exercise 3 - Train/evaluate the network\n",
        "Train the network you built using the code below. First check your code by training a single epoch, which should already give you around 50% train accuracy. Then run it for 100-200 epochs such that the code converges.\n",
        "\n",
        "Add the following answers in your report:\n",
        "1. What test accuracy were you able to get?\n",
        "2. How many layers does your network have? (counting only convolutional and dense layers)\n",
        "3. Why do the skip connections help for training deep neural networks?\n",
        "4. What options do you have to improve the test accuracy? Explain 3 options why you think that they would improve accuracy. (you do not need to implement/code them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcG_bfjoT7Dx"
      },
      "source": [
        "start=time.time()\n",
        "\n",
        "for epoch in range(0,200):\n",
        "\n",
        "  net.train()  # Put the network in train mode\n",
        "  for i, (x_batch, y_batch) in enumerate(trainloader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n",
        "\n",
        "    optimizer.zero_grad()  # Set all currenly stored gradients to zero\n",
        "\n",
        "    y_pred = net(x_batch)\n",
        "\n",
        "    loss = criterion(y_pred, y_batch)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute relevant metrics\n",
        "\n",
        "    y_pred_max = torch.argmax(y_pred, dim=1)  # Get the labels with highest output probability\n",
        "\n",
        "    correct = torch.sum(torch.eq(y_pred_max, y_batch)).item()  # Count how many are equal to the true labels\n",
        "\n",
        "    elapsed = time.time() - start  # Keep track of how much time has elapsed\n",
        "\n",
        "    # Show progress every 20 batches\n",
        "    if not i % 20:\n",
        "      print(f'epoch: {epoch}, time: {elapsed:.3f}s, loss: {loss.item():.3f}, train accuracy: {correct / batch_size:.3f}')\n",
        "\n",
        "    correct_total = 0\n",
        "\n",
        "  net.eval()  # Put the network in eval mode\n",
        "  for i, (x_batch, y_batch) in enumerate(testloader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n",
        "\n",
        "    y_pred = net(x_batch)\n",
        "    y_pred_max = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "    correct_total += torch.sum(torch.eq(y_pred_max, y_batch)).item()\n",
        "\n",
        "  print(f'Accuracy on the test set: {correct_total / len(testset):.3f}')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jcJkRe8T7BW"
      },
      "source": [
        "correct_total = 0\n",
        "\n",
        "for i, (x_batch, y_batch) in enumerate(testloader):\n",
        "  x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move the data to the device that is used\n",
        "\n",
        "  y_pred = net(x_batch)\n",
        "  y_pred_max = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "  correct_total += torch.sum(torch.eq(y_pred_max, y_batch)).item()\n",
        "\n",
        "print(f'Accuracy on the test set: {correct_total / len(testset):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}